{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From IN22k, get IN6k and create miniIN6k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all IN22k classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21841"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "# total classes in IN22k folder: 21841\n",
    "# classes not in IN1k: 20842\n",
    "# These 20842 classes have a total of 12.906.958 images\n",
    "IMAGENET22K_DIR = '/datasets01_101/imagenet-22k/062717/'\n",
    "all_ = glob(IMAGENET22K_DIR+'*')\n",
    "classes_dir_22k = glob(IMAGENET22K_DIR+'n*.tar')\n",
    "other_file = [x for x in all_ if x not in classes_dir_22k]\n",
    "\n",
    "classes_22k = [x.split('/')[-1].split('.')[0] for x in classes_dir_22k]\n",
    "len(classes_22k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get clean images and classes, cleaned automatically to remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21783"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLEAN_IMAGE_LIST = 'data/IN6k/clean_images.txt' # 11795291 images filtered by Matthijs \n",
    "with open(CLEAN_IMAGE_LIST) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "allclasses = [line.split('_')[0] for line in lines]\n",
    "clean_classes = list(set(allclasses)) # from 21841 keep only 21783\n",
    "len(clean_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get IN1k classes, in order to avoid them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images to use later among clean classes, and discard ones not in this dict\n",
    "- Avoid classes from IN1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21394"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "IN1k_classes = torch.load('data/IN6k/IN1k_classes.pth') # load IN1k list of classes names\n",
    "classes_not_in_IN1k = [x for x in clean_classes if x not in IN1k_classes]\n",
    "len(classes_not_in_IN1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11795291it [01:08, 171232.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "clean_class_to_im_dict = dict([(c,[]) for c in classes_not_in_IN1k])\n",
    "for (c, line) in tqdm(zip(allclasses, lines)):\n",
    "# for line in lines:\n",
    "#     c = line.split('_')[0]\n",
    "    if c in IN1k_classes:\n",
    "        continue\n",
    "    clean_class_to_im_dict[c].append(line.replace('\\n',''))\n",
    "# takes time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tar dataset of each class not in IN1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "for c in classes_not_in_IN1k:\n",
    "    f = f'/checkpoint/matthijs/imagenet-22k/tarindex/{c}.tarlog'\n",
    "    shutil.copyfile(f, f'data/IN6k/tars/{c}.tarlog')\n",
    "    print(os.path.exists(f))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/private/home/sbaio/spicy-lorikeet/')\n",
    "\n",
    "from imagenet.imagenet22k import TarDataset\n",
    "IMAGENET22K_DIR = '/datasets01_101/imagenet-22k/062717/'\n",
    "i22ktarlogs = '/checkpoint/matthijs/imagenet-22k/tarindex/'\n",
    "\n",
    "tar_dsets = {}\n",
    "for c in classes_not_in_IN1k:\n",
    "    tar_dataset = TarDataset(IMAGENET22K_DIR + c + '.tar',\n",
    "                            i22ktarlogs + c + '.tarlog',\n",
    "                            preload=True)\n",
    "    tar_dsets[c] = tar_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe51d53be10>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "l = []\n",
    "for c in classes_not_in_IN1k:\n",
    "    l.append(len(clean_class_to_im_dict[c]))\n",
    "l = sorted(l)\n",
    "plt.plot(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2248"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted_classes_not_in_IN1k = sorted(list(tar_dsets.items()), key=lambda x:len(x[1]))\n",
    "sorted_classes_not_in_IN1k = sorted(list(clean_class_to_im_dict.items()), key=lambda x:len(x[1]))\n",
    "len(sorted_classes_not_in_IN1k[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6056"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classes_with_more_than_1k_images = sorted(list(filter(lambda x:len(x[1])>1000, list(tar_dsets.items()))), key=lambda x:len(x[1]), reverse=True)\n",
    "classes_with_more_than_1k_images = sorted(list(filter(lambda x:len(x[1])>=900, list(clean_class_to_im_dict.items()))), key=lambda x:len(x[1]), reverse=True)\n",
    "len(classes_with_more_than_1k_images)\n",
    "# allow taking less than 1000 images ... to have 6k classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "907"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes_with_more_than_1k_images[:6000][-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2248 907\n"
     ]
    }
   ],
   "source": [
    "largest_6k_classes = classes_with_more_than_1k_images[:6000]\n",
    "print(len(largest_6k_classes[0][1]), len(largest_6k_classes[-1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check intersection with IN1k classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x[0] for x in largest_6k_classes]).intersection(IN1k_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7135116"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected size of IN6k excluding IN1k\n",
    "sum([len(x[1]) for x in largest_6k_classes])\n",
    "\n",
    "# compared to 7791368 before removing duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a dict of class name to imlist, avoid using tar_dset imlist, because it contains non clean images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_6k_classes_dict = dict(largest_6k_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/private/home/sbaio/spicy-lorikeet/')\n",
    "from imagenet.imagenet22k import TarDataset\n",
    "IMAGENET22K_DIR = '/datasets01_101/imagenet-22k/062717/'\n",
    "i22ktarlogs = '/checkpoint/matthijs/imagenet-22k/tarindex/'\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "try:\n",
    "    from StringIO import StringIO as DataIO\n",
    "except ImportError:\n",
    "    from io import BytesIO as DataIO\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "\n",
    "IN_resize = T.Resize((256))\n",
    "mini_resize = T.Resize((84,84))\n",
    "\n",
    "image_size = 84\n",
    "IN6k_dst = '/checkpoint/sbaio/IN6k2/'\n",
    "miniIN6k_dst = '/checkpoint/sbaio/miniIN6k2/'\n",
    "\n",
    "def copy_class(c, largest_6k_classes_dict):\n",
    "    def process_im(tar_dset, c, imname, dst_dir, minidst_dir):\n",
    "        import os\n",
    "        from PIL import Image\n",
    "        try:\n",
    "            from StringIO import StringIO as DataIO\n",
    "        except ImportError:\n",
    "            from io import BytesIO as DataIO\n",
    "        from torchvision import transforms as T\n",
    "\n",
    "        IN_resize = T.Resize((256))\n",
    "        mini_resize = T.Resize((84,84))\n",
    "        dst_file = os.path.join(dst_dir, c, imname.split('.')[0]+'.jpg')\n",
    "        minidst_file = os.path.join(minidst_dir, c, imname.split('.')[0]+'.jpg')\n",
    "        if os.path.exists(dst_file) and os.path.exists(minidst_file):\n",
    "            return\n",
    "        data = tar_dset.get_name(imname)\n",
    "        try:\n",
    "            im = Image.open(DataIO(data))\n",
    "        except Exception as e:\n",
    "            print(\"Error im %s, %s\" % (imname, e))\n",
    "            im = Image.new('RGB', (256, 256))\n",
    "        im = im.convert('RGB')\n",
    "        im = IN_resize(im)\n",
    "        im.save(dst_file)\n",
    "\n",
    "        im = mini_resize(im)\n",
    "        im.save(minidst_file)\n",
    "    import time\n",
    "    import sys\n",
    "    import os\n",
    "    sys.path.insert(0,'/private/home/sbaio/spicy-lorikeet/')\n",
    "\n",
    "    from imagenet.imagenet22k import TarDataset\n",
    "    IMAGENET22K_DIR = '/datasets01_101/imagenet-22k/062717/'\n",
    "    i22ktarlogs = '/checkpoint/matthijs/imagenet-22k/tarindex/'\n",
    "    \n",
    "    IN6k_dst = '/checkpoint/sbaio/IN6k2/'\n",
    "    miniIN6k_dst = '/checkpoint/sbaio/miniIN6k2/'\n",
    "    \n",
    "    start = time.time()\n",
    "    tar_dataset = TarDataset(IMAGENET22K_DIR + c + '.tar',\n",
    "                        i22ktarlogs + c + '.tarlog',\n",
    "                        preload=True)\n",
    "    for dst_dir in [IN6k_dst, miniIN6k_dst]:\n",
    "        cdir = dst_dir+'{}'.format(c)\n",
    "        if not os.path.exists(cdir):\n",
    "            os.mkdir(cdir)\n",
    "\n",
    "    for i,imname in enumerate(largest_6k_classes_dict[c]):#tar_dataset.names):\n",
    "        process_im(tar_dataset, c, imname, dst_dir=IN6k_dst, minidst_dir=miniIN6k_dst)\n",
    "    msg = 'Took {:0.2f}, Copied {} images of class {}'.format(time.time()-start, i+1, c)\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_to_copy = [x[0] for x in largest_6k_classes]\n",
    "len(classes_to_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from concurrent import futures\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", \"(Possibly )?corrupt EXIF data\", UserWarning)\n",
    "# N = 0\n",
    "# print('Copying {} classes'.format(len(classes_to_copy)))\n",
    "# # We can use a with statement to ensure threads are cleaned up promptly\n",
    "# with futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "#     # Start the load operations and mark each future with its arg\n",
    "#     future_to_url = {executor.submit(copy_class, i): i for i in classes_to_copy}\n",
    "#     for future in futures.as_completed(future_to_url):\n",
    "#         url = future_to_url[future]\n",
    "#         try:\n",
    "#             ret = future.result()\n",
    "#         except Exception as exc:\n",
    "#             print('%r generated an exception: %s' % (url, exc))\n",
    "#         else:\n",
    "#             N += 1\n",
    "#             print('{} : {}'.format(N, ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import submitit\n",
    "executor = submitit.AutoExecutor(folder='/checkpoint/sbaio/jobs/copy_miniIN/%j')\n",
    "executor.update_parameters(timeout_min=100, partition='learnfair', constraint=\"volta\",\n",
    "                           tasks_per_node=1,gpus_per_node=1, mem_gb=100, \n",
    "                           cpus_per_task=10, nodes=1, signal_delay_s=120)\n",
    "# jobs = []\n",
    "max_running_jobs = 200\n",
    "i = 0\n",
    "for c in classes_to_copy[::-1]:\n",
    "    running = [job for job in jobs if job.state=='RUNNING']\n",
    "    failed = [job for job in jobs if job.state=='FAILED']\n",
    "    completed = [job for job in jobs if job.state=='COMPLETED']\n",
    "    pending = [job for job in jobs if job.state=='PENDING']\n",
    "    unknown = [job for job in jobs if job.state=='UNKNOWN']\n",
    "\n",
    "    if len(failed) > 0:\n",
    "        print('Some jobs failed')\n",
    "        break\n",
    "    while len(running)+len(pending)+len(unknown)>=max_running_jobs:\n",
    "        time.sleep(0.1)\n",
    "        i+=1\n",
    "        if i % 10 == 0:\n",
    "            print('Waiting for some jobs to finish, completed {}'.format(len(completed)))\n",
    "\n",
    "    job = executor.submit(copy_class, c, largest_6k_classes_dict)\n",
    "    jobs.append(job)\n",
    "    print('Launched job: {}'.format(job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in jobs:\n",
    "    job.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute mean and std of the resulting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset\n",
    "from torchvision import transforms as T\n",
    "dset = get_dataset('miniIN6k','train', no_transform=True)\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "dset.transform = transform\n",
    "# dset = get_dataset('IN1k','train', transform=T.Compose([T.Resize((256,256)),T.ToTensor()]))\n",
    "#\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(\n",
    "    dset,\n",
    "    batch_size=2000,\n",
    "    num_workers=80,\n",
    "    shuffle=False#, pin_memory=True\n",
    ")\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "nb_samples = 0.\n",
    "print(len(loader))\n",
    "start = time.time()\n",
    "for i,(batch,_) in enumerate(loader):\n",
    "    batch_samples = batch.size(0)\n",
    "    batch = batch.view(batch_samples, batch.size(1), -1)\n",
    "    mean += batch.mean(2).sum(0)\n",
    "    std += batch.std(2).sum(0)\n",
    "    nb_samples += batch_samples\n",
    "    if i%10==0:\n",
    "        print(i, '{:0.2f}'.format(time.time()-start))\n",
    "        print(mean/nb_samples, std/nb_samples)\n",
    "        start = time.time()\n",
    "\n",
    "mean /= nb_samples\n",
    "std /= nb_samples\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2158, -0.2202, -0.1855])\n",
      "tensor([1.1501, 1.1323, 1.0899])\n"
     ]
    }
   ],
   "source": [
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and cache it\n",
    "from torchvision.datasets import ImageFolder\n",
    "miniIN6k = ImageFolder('/checkpoint/sbaio/miniIN6k_clean/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7135116 6000\n"
     ]
    }
   ],
   "source": [
    "print(len(miniIN6k), len(miniIN6k.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cache.\n"
     ]
    }
   ],
   "source": [
    "dset = miniIN6k\n",
    "tosave = {\n",
    "    'classes':dset.classes,\n",
    "    'class_to_idx':dset.class_to_idx,\n",
    "    'samples':dset.samples\n",
    "}\n",
    "# torch.save(tosave, '/private/home/sbaio/.cache/miniIN6k_clean.bin')\n",
    "# print('Saved cache.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
